{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3df43403-7733-4fb4-bc54-8a74b851a93a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### <span style=\"color:lightgray\">EDC, November 2024</span>\n",
    "\n",
    "# Developing solutions with LLMs\n",
    "---\n",
    "\n",
    "### Matt Hall, Equinor &nbsp; `mtha@equinor.com`\n",
    "\n",
    "<span style=\"color:lightgray\">&copy;2024  Matt Hall, Equinor &nbsp; | &nbsp; licensed CC BY, please share this work</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b521eb77-af13-4916-b5b7-b2723c02a1c7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# See https://platform.openai.com/docs/quickstart\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "__ = load_dotenv(\"secrets.txt\") # If key is in a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6869dfda-1fb4-4bb7-8bca-b431ca98f598",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "import httpx\n",
    "import base64\n",
    "\n",
    "\n",
    "MODEL = \"gpt-35-turbo\" # Deployment name; \"gpt-4o\" is multimodal.\n",
    "\n",
    "CLIENT = AzureOpenAI(\n",
    "    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_KEY\"),  \n",
    "    api_version=\"2024-02-01\",\n",
    ")\n",
    "\n",
    "def ask(prompt, model=MODEL, image_url=None):\n",
    "    \"\"\"Ask ChatGPT about an (optional) image.\"\"\"\n",
    "    content = []\n",
    "\n",
    "    if image_url is not None:\n",
    "        image_media_type = f\"image/{image_format}\"\n",
    "        image = base64.b64encode(httpx.get(image_url).content).decode(\"utf-8\")\n",
    "        image_content = {\n",
    "              \"type\": \"image_url\",\n",
    "              \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"}\n",
    "            }\n",
    "        content.append(image_content)\n",
    "\n",
    "    content.append({\"type\": \"text\", \"text\": prompt})\n",
    "    \n",
    "    messages = [{\"role\": \"user\", \"content\": content},]\n",
    "    response = CLIENT.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0.5,\n",
    "        max_tokens=1024,\n",
    "        messages=messages\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def tokenize(prompt):\n",
    "    encoding = tiktoken.encoding_for_model(MODEL)\n",
    "    tokens = encoding.encode(prompt)\n",
    "    decode = lambda token: encoding.decode_single_token_bytes(token).decode()\n",
    "    return [decode(token) for token in tokens]\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-3-large\"):\n",
    "   text = text.replace(\"\\n\", \" \")\n",
    "   return CLIENT.embeddings.create(input=[text], model=model).data[0].embedding\n",
    "\n",
    "\n",
    "class Convo:\n",
    "    def __init__(self, temperature=0, model='gpt-35-turbo'):\n",
    "        self.temperature = temperature\n",
    "        self.model = model\n",
    "        self.messages = []\n",
    "\n",
    "    def ask(self, prompt):\n",
    "        self.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "        response = CLIENT.chat.completions.create(\n",
    "            model=self.model,\n",
    "            temperature=self.temperature,\n",
    "            max_tokens=1024,\n",
    "            messages=self.messages\n",
    "        )\n",
    "        content = response.choices[0].message.content\n",
    "        self.messages.append({'role': 'assistant',  'content': content})\n",
    "        return content\n",
    "\n",
    "    def history(self):\n",
    "        return self.messages\n",
    "\n",
    "# Needed for f-string printing later.\n",
    "n = '\\n'\n",
    "\n",
    "# Check that things work.\n",
    "ask('Repeat exactly: ✅ System check')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b63ecf-bbb0-44fd-bdb8-1c61bea7cfb5",
   "metadata": {},
   "source": [
    "## Can agents help?\n",
    "\n",
    "LLMs cannot answer questions like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d308293d-1779-4a4d-85d1-79d0f09c3575",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = (\"What is the Gardner equation's prediction \"\n",
    "     \"of density if Vp is 2000 m/s? \"\n",
    "     \"Assume a = 0.31 and b = 0.25. \"\n",
    "     \"Think step by step.\")\n",
    "\n",
    "print(ask(q))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fd5d9d-ae87-46bd-9bd0-55694bd9a5e9",
   "metadata": {},
   "source": [
    "Hmm...\n",
    "\n",
    "---\n",
    "\n",
    "Let's ask the smarter model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18daa07a-ec6e-45ff-885b-2575e9e1eed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = (\"What is the Gardner equation's prediction \"\n",
    "     \"of density if Vp is 2000 m/s? \"\n",
    "     \"Assume a = 0.31 and b = 0.25. \"\n",
    "     \"Think step by step.\")\n",
    "\n",
    "print(ask(q, model='gpt-4o'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3938a71-a9e1-447a-a6f7-ec7aef343c50",
   "metadata": {},
   "source": [
    "Still no good.\n",
    "\n",
    "---\n",
    "\n",
    "Let's try something else."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932fec12-7829-458f-966b-cde9c8b17787",
   "metadata": {},
   "source": [
    "**Agents** can provide services:\n",
    "\n",
    "- Maths\n",
    "- Search\n",
    "- Code execution\n",
    "- API calls\n",
    "- Database queries\n",
    "\n",
    "For example, a **math agent** can answer mathematical questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd591cf-de4f-447d-90e9-d3f91df57142",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import load_tools\n",
    "from langchain_openai import AzureChatOpenAI as ACOAI\n",
    "from langchain.agents import AgentType\n",
    "\n",
    "llm = ACOAI(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_KEY\"),  \n",
    "    api_version=\"2024-02-01\",\n",
    "    model=\"gpt-35-turbo\",\n",
    ")\n",
    "\n",
    "agent = initialize_agent(\n",
    "    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    llm=llm,\n",
    "    tools=load_tools(['llm-math'], llm=llm),\n",
    "    handle_parsing_errors=True,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "agent.invoke(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb04c8b-9c0e-4e8f-b310-cd09194ff210",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## RAG\n",
    "\n",
    "Retrieval-augmented generation is another approach to keeping an LLM's information on rails. We first find documents that are semantically similar to the query prompt, inject those into the prompt we give to the LLM, and tell it to constrain its response to information from those documents.\n",
    "\n",
    "The approach depends on comparing embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59b5d85-1baf-49dc-9ba0-e4778cfd09cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Describe the rocks in Ainsa.\"\n",
    "\n",
    "ask(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d10e6bf-8214-466c-a777-a136f190f183",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = (\"Sandstones in the Ainsa basin are \"\n",
    "        \"generally composed of carbonate grains.\")\n",
    "\n",
    "e = get_embedding(text)  # 8192 tokens, 1536 dimensions\n",
    "\n",
    "len(e), e[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581ba2e7-7d76-4449-9378-f92bc0aa389c",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    \"Sandstones in the Ainsa basin are \"\n",
    "    \"generally composed of carbonate grains.\",\n",
    "\n",
    "    \"Siltstones in the Ainsa basin have extensive \"\n",
    "    \"early carbonate cementation.\",\n",
    "\n",
    "    \"The rocks in the Ainsa Basin are generally \"\n",
    "    \"Eocene in age.\",\n",
    "\n",
    "    \"The rocks in the Tremp Basin are generally \"\n",
    "    \"Cretaceous in age.\",\n",
    "\n",
    "    \"Arsenal’s only loss in their last nine games \"\n",
    "    \"was in the first leg.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ce865a-f3e7-4199-988e-4760ee3341da",
   "metadata": {},
   "source": [
    "Now I have lots of docs, I need a way to decide how similar 2 docs are. Here's a popular one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94347684-419a-414f-a959-e2cd022908b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine(u, v):\n",
    "    \"\"\"Cosine similarity between two vectors\"\"\"\n",
    "    return np.dot(u, v) / (norm(u) * norm(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50625568-6571-4676-a6f7-90592e6eee45",
   "metadata": {},
   "source": [
    "Compute the similarities between my query and the docs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defef380-54cd-456f-932d-bbafc4b4442a",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = get_embedding(query)\n",
    "\n",
    "sims = []\n",
    "for idx, doc in enumerate(docs):\n",
    "    x = get_embedding(doc)\n",
    "    sims.append(cosine(q, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d68f2e-8a94-4ba2-aa5e-1fbdaaac5ae8",
   "metadata": {},
   "source": [
    "Look at the similarities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8461a03f-ca41-47cf-ab4a-9e49cccf3aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{query}\\n{'='*len(query)}\")\n",
    "for doc, sim in zip(docs, sims):\n",
    "    print(f\"{doc[:29]}... {sim:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3debe237-18f9-4a2f-9a81-e270b56dcf43",
   "metadata": {},
   "source": [
    "Answer the question with the useful docs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60d0efd-bc3d-4572-80c4-c7187efd691e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Who is the king of Spain?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d74c50-addf-403c-92d1-4cbafee36c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = []\n",
    "q = get_embedding(query)\n",
    "\n",
    "for idx, doc in enumerate(docs):\n",
    "    x = get_embedding(doc)\n",
    "    sims.append(cosine(q, x))\n",
    "    \n",
    "useful = [d for d, s in zip(docs, sims) if s > 0.2]\n",
    "\n",
    "prompt = (f\"Answer this question:\\n\\n> {query}\\n\\nIMPORTANT: Use the following information only:\\n\\n\"\n",
    "    f\"{n.join(useful)}\\n\\nIf the documents are not relevant, use your implicit knownledge to answer instead.\")\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e029596-d452-4faa-a0e9-5663a39b1a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ask(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128532a9-42ee-45c8-aa6f-5c8d8cedd91a",
   "metadata": {},
   "source": [
    "There are still plenty of questions about how best to do this:\n",
    "\n",
    "- How to chunk the documents?\n",
    "- How to compare the prompt?\n",
    "- How to know when to look for documents?\n",
    "- How to constrain the response to the retrieved docs?\n",
    "- How to do all this efficiently?\n",
    "\n",
    "## Gotcha\n",
    "\n",
    "Let's add another document from our source.\n",
    "\n",
    ">3. West of the Mediano Anticline\n",
    ">\n",
    ">Everywhere in the Pyrenees, except in the Ainsa Basin, fractures play an important role in the diagenesis.\n",
    "\n",
    "It's long, so we split it into 2 pieces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03dbc3a0-3aed-4d93-845e-58f20270091c",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs.extend([\n",
    "    \"3. West of the Mediano Anticline\\nEverywhere \"\n",
    "    \"in the Pyrenees, except\",\n",
    "    \"in the Ainsa Basin, fractures play an \"\n",
    "    \"important role in the diagenesis.\",\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf29295-9c41-4fa6-8394-c4756b5b9def",
   "metadata": {},
   "source": [
    "Let's ask a new question, this time about diagenesis.\n",
    "\n",
    "We're looking for docs with high similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeec68e8-e8f0-414b-8e50-cf616b8bdf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = (\"Summarize the diagenesis in \"\n",
    "         \"the Ainsa Basin.\")\n",
    "q = get_embedding(query)\n",
    "\n",
    "sims = []\n",
    "for idx, doc in enumerate(docs):\n",
    "    x = get_embedding(doc)\n",
    "    sims.append(cosine(q, x))\n",
    "\n",
    "print(f\"{query}\\n{'='*len(query)}\")\n",
    "for doc, sim in zip(docs, sims):\n",
    "    print(f\"{doc[:29]}... {sim:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0f3a36-4188-4a0f-b76a-b4882961c2ef",
   "metadata": {},
   "source": [
    "Now answer the question, using the retreived documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e478bde-6dbf-478d-a03a-8fdba498a376",
   "metadata": {},
   "outputs": [],
   "source": [
    "ask(f\"{query}\\nUse the following information only:\\n\"\n",
    "    f\"{n.join([d for d, s in zip(docs, sims) if s > 0.5])}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb95aa2b-98aa-4d80-b9f1-1709b4e613ed",
   "metadata": {},
   "source": [
    "Uh oh! Looks like we need more people to work on this problem..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca9b4ba-7dd2-44e5-b1d0-a0b4fbefc9d7",
   "metadata": {},
   "source": [
    "<span style=\"color:lightgray\">&copy; 2024 Matt Hall, Equinor &nbsp; | &nbsp; licensed CC BY, please share this work</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
